# ğŸ›’ Global Store Data Pipeline (ETL)

Pipeline de dados automatizado (End-to-End) para extraÃ§Ã£o, transformaÃ§Ã£o e carga de produtos de e-commerce. Desenvolvido para processar dados da FakeStoreAPI e popular um Data Warehouse na nuvem utilizando a **Arquitetura de MedalhÃ£o**.

---

## ğŸš€ Tecnologias e Ferramentas

* **Linguagem:** Python 3.12
* **ManipulaÃ§Ã£o e Tratamento:** Pandas
* **Banco de Dados Cloud:** PostgreSQL (Render)
* **ConexÃ£o / ORM:** SQLAlchemy
* **OrquestraÃ§Ã£o de Dados:** Apache Airflow
* **GestÃ£o de DependÃªncias:** Poetry

---

## ğŸ—ï¸ Arquitetura do Projeto

O pipeline foi modularizado em etapas claras para seguir as melhores prÃ¡ticas de Engenharia de Dados:

1. **Setup de Infraestrutura (`init_db.py`)**
   Garante a criaÃ§Ã£o explÃ­cita da tabela `silver_products` no PostgreSQL com as tipagens corretas (DDL) antes de qualquer carga de dados.

2. **Camada Bronze (ExtraÃ§Ã£o - `api_client.py`)**
   Consumo de dados via API REST com adiÃ§Ã£o automÃ¡tica de `extraction_timestamp` (auditoria) e tratamento de falhas de rede (timeouts).

3. **Camada Silver (TransformaÃ§Ã£o - `transform.py`)**
   Limpeza de dados e *flattening* (achatamento) dinÃ¢mico de estruturas JSON aninhadas (`rating`) utilizando a alta performance do Pandas.

4. **Carga (`db_manager.py`)**
   PersistÃªncia dos dados estruturados no Data Warehouse utilizando prÃ¡ticas seguras de conexÃ£o via variÃ¡veis de ambiente.

5. **OrquestraÃ§Ã£o (`dags/global_store_dag.py`)**
   Fluxo estruturado em uma DAG do Airflow, com isolamento de tarefas (**Setup â†’ Extract â†’ Transform â†’ Load**) e comunicaÃ§Ã£o de metadados via XCom.

---

## âš™ï¸ Como Executar Localmente (Standalone)

Para testar o fluxo de extraÃ§Ã£o e carga no banco de dados localmente (sem a necessidade de subir os containers do Airflow), vocÃª pode utilizar o orquestrador embutido `main.py`.

### ğŸ“‹ PrÃ©-requisitos

* Python 3.12+
* Poetry instalado:

```bash
pip install poetry
```

---

### â–¶ï¸ Passo a Passo

#### **1. Clone o repositÃ³rio**

```bash
git clone https://github.com/samuelZ20/global_store_pipeline.git
cd global_store_pipeline
```

#### **2. Instale as dependÃªncias com o Poetry**

```bash
poetry install
```

#### **3. Configure as VariÃ¡veis de Ambiente**

Crie um arquivo chamado `.env` na raiz do projeto e adicione as credenciais do seu banco PostgreSQL no Render:

```env
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
DB_HOST=seu_host.render.com
DB_NAME=seu_banco
```

#### **4. Execute o Pipeline Completo**

O comando abaixo validarÃ¡ o banco de dados (criando a tabela se necessÃ¡rio) e farÃ¡ o ciclo completo de ETL:

```bash
poetry run python main.py
```

---

## ğŸŒ¬ï¸ ExecuÃ§Ã£o via Apache Airflow

A lÃ³gica de orquestraÃ§Ã£o distribuÃ­da encontra-se no diretÃ³rio:

```
dags/global_store_dag.py
```

A DAG foi construÃ­da utilizando `PythonOperator` e estÃ¡ pronta para ser:

* Acoplada a qualquer ambiente Airflow
* Agendada (`@daily`)
* Executada em ambientes containerizados (Docker, Astro CLI, etc.)

---

## ğŸ‘¨â€ğŸ’» Autor

**Samuel Frizzone Cardoso**
Engenharia de Dados â€” UFLA
