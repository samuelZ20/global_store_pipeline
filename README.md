# ğŸ›’ Global Store ETL Pipeline

Este projeto Ã© um **pipeline de dados automatizado** que extrai informaÃ§Ãµes de produtos da **FakeStoreAPI**, processa os dados com **Pandas** e os armazena em um **Data Warehouse PostgreSQL (Render)**.
A orquestraÃ§Ã£o Ã© realizada via **Apache Airflow**, garantindo monitoramento e reexecuÃ§Ã£o segura (**idempotÃªncia**).

---

## ğŸ‘¤ Autor

**Samuel Frizzone Cardoso**

---

## ğŸš€ Tecnologias

* **Linguagem:** Python 3.12
* **OrquestraÃ§Ã£o:** Apache Airflow 2.11+
* **TransformaÃ§Ã£o:** Pandas
* **Banco de Dados:** PostgreSQL (Render)
* **Gerenciador de DependÃªncias:** Poetry

---

## ğŸ“‚ Estrutura do Projeto

```plaintext
global_store_pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ global_store_dag.py     # OrquestraÃ§Ã£o do fluxo de tarefas
â”œâ”€â”€ src/                        # NÃºcleo da lÃ³gica (Modules)
â”‚   â”œâ”€â”€ api_client.py           # ExtraÃ§Ã£o (Bronze)
â”‚   â”œâ”€â”€ transform.py            # TransformaÃ§Ã£o (Silver)
â”‚   â”œâ”€â”€ db_manager.py           # ConexÃ£o com o Banco
â”‚   â””â”€â”€ init_db.py              # DDL e InicializaÃ§Ã£o
â”œâ”€â”€ main.py                     # ExecuÃ§Ã£o Manual/Debug
â”œâ”€â”€ pyproject.toml              # ConfiguraÃ§Ãµes do Poetry
â””â”€â”€ .env                        # VariÃ¡veis SensÃ­veis (NÃ£o versionado)
```

---

## ğŸ› ï¸ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o

### 1ï¸âƒ£ Clonar o RepositÃ³rio

Abra o seu terminal (preferencialmente WSL/Ubuntu) e baixe o projeto:

```bash
git clone https://github.com/samuelZ20/global_store_pipeline.git
cd global_store_pipeline
```

---

### 2ï¸âƒ£ Instalar DependÃªncias

Utilize o Poetry para criar o ambiente virtual e instalar as bibliotecas:

```bash
poetry install
```

---

### 3ï¸âƒ£ Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com as credenciais do seu banco no Render:

```env
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
DB_HOST=seu_host_no_render.com
DB_NAME=global_store_dw
```

---

## ğŸƒ Como Executar

### ğŸ”¹ Modo Local (Teste RÃ¡pido)

Valida a lÃ³gica ETL e a persistÃªncia no banco **sem a interface do Airflow**:

```bash
poetry run python main.py
```

---

### ğŸ”¹ Modo Orquestrado (Airflow Standalone)

Para rodar com **agendamento e monitoramento visual**:

#### 1. Vincular DAGs e MÃ³dulos

Configure o Airflow para reconhecer a pasta do projeto:

```bash
mkdir -p ~/airflow/dags
ln -s $(pwd)/dags/* ~/airflow/dags/
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

#### 2. Iniciar Airflow

```bash
poetry run airflow standalone
```

#### 3. Acesso

Abra no navegador:

```
http://localhost:8080
```

FaÃ§a login com as credenciais geradas no terminal e ative a DAG **global_store_multi_task_pipeline**.

---

## ğŸ§  DecisÃµes TÃ©cnicas

### âœ… Carga Robusta

A persistÃªncia utiliza **SQLAlchemy Core (Bulk Insert)** para evitar incompatibilidades de drivers entre o Pandas e o ambiente local.

### âœ… IdempotÃªncia

O uso de `TRUNCATE` em transaÃ§Ãµes atÃ´micas (`engine.begin()`) garante que falhas no meio do processo nÃ£o deixem dados duplicados ou inconsistentes no Data Warehouse.

---

## âœ… Objetivo

Demonstrar a construÃ§Ã£o de um pipeline ETL moderno com:

* OrquestraÃ§Ã£o profissional
* SeparaÃ§Ã£o em camadas (Bronze â†’ Silver)
* IntegraÃ§Ã£o com Data Warehouse na nuvem
* Boas prÃ¡ticas de engenharia de dados
