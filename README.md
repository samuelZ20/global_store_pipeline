# Global Store ETL Pipeline

Pipeline de dados automatizado que extrai produtos da **FakeStoreAPI**, transforma com **Pandas** e armazena em um **Data Warehouse PostgreSQL (Render)**. Orquestrado via **Apache Airflow** com uma DAG por camada.

---

## ğŸ‘¤ Autor

**Samuel Frizzone Cardoso**

---

## ğŸš€ Tecnologias

| Tech | VersÃ£o |
|---|---|
| Python | 3.12 |
| Apache Airflow | 2.9+ |
| Pandas | 2.0+ |
| SQLAlchemy | 1.4 |
| PostgreSQL | Render Cloud |
| Poetry | Gerenciador de deps |

---

## ğŸ—ï¸ Arquitetura

```mermaid
flowchart TD
    A([FakeStoreAPI]) -->|GET /products| B

    subgraph Airflow ["Apache Airflow â€” OrquestraÃ§Ã£o"]
        direction TB

        DAG0[dag_setup_db\n@once]:::infra
        DAG1[dag_check_connection\n@hourly]:::infra
        DAG2[dag_bronze_extract\n@daily]:::bronze
        DAG3[dag_silver_transform\n@daily]:::silver
        DAG4[dag_load_validate\n@daily]:::load

        DAG0 -.->|prÃ©-requisito| DAG2
        DAG1 -.->|monitoramento| DAG2
    end

    B[dag_bronze_extract] --> C[(bronze_products\nJSON bruto)]
    C --> D[dag_silver_transform]
    D --> E[(silver_products\nDados limpos)]
    E --> F[dag_load_validate]
    F --> G([BI / APIs Externas])

    classDef infra fill:#6c757d,color:#fff,stroke:none
    classDef bronze fill:#cd7f32,color:#fff,stroke:none
    classDef silver fill:#aaa,color:#fff,stroke:none
    classDef load fill:#198754,color:#fff,stroke:none
```

---

## ğŸ“‚ Estrutura do Projeto

```plaintext
global_store_pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_setup_db.py          # [INFRA] Cria as tabelas (rodar primeiro, @once)
â”‚   â”œâ”€â”€ dag_check_connection.py  # [INFRA] Verifica conectividade (@hourly)
â”‚   â”œâ”€â”€ dag_bronze_extract.py    # [ETL]   API â†’ bronze_products (@daily)
â”‚   â”œâ”€â”€ dag_silver_transform.py  # [ETL]   bronze â†’ silver_products (@daily)
â”‚   â””â”€â”€ dag_load.py              # [ETL]   ValidaÃ§Ã£o e exposiÃ§Ã£o final (@daily)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api_client.py            # ExtraÃ§Ã£o da FakeStoreAPI
â”‚   â”œâ”€â”€ transform.py             # TransformaÃ§Ãµes Pandas (flattening, tipagem)
â”‚   â”œâ”€â”€ db_manager.py            # Engine SQLAlchemy + conectividade
â”‚   â”œâ”€â”€ init_db.py               # DDL das tabelas Bronze e Silver
â”‚   â””â”€â”€ utils.py                 # Helper genÃ©rico de persistÃªncia
â”œâ”€â”€ bootstrap.sh                 # Setup do ambiente local
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env                         # Credenciais (nÃ£o versionado)
```

---

## ğŸ› ï¸ InstalaÃ§Ã£o

### 1. Clonar o repositÃ³rio

```bash
git clone https://github.com/samuelZ20/global_store_pipeline.git
cd global_store_pipeline
```

### 2. Instalar dependÃªncias

```bash
poetry install
```

### 3. Configurar o `.env`

```env
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
DB_HOST=seu_host.oregon-postgres.render.com
DB_NAME=nome_do_banco
DB_PORT=5432
```

---

## ğŸƒ Como Executar

### 1. Bootstrap do ambiente

```bash
source bootstrap.sh
```

O script configura o `PYTHONPATH`, cria os symlinks das DAGs em `~/airflow/dags` e carrega o `.env` automaticamente.

### 2. Iniciar o Airflow

```bash
poetry run airflow standalone
```

Acesse `http://localhost:8080` com as credenciais exibidas no terminal (tambÃ©m salvas em `~/airflow/standalone_admin_password.txt`).

### 3. Ordem de execuÃ§Ã£o das DAGs

| Ordem | DAG ID | DescriÃ§Ã£o |
|:---:|---|---|
| 1ï¸âƒ£ | `setup_database` | Cria as tabelas â€” rodar **uma Ãºnica vez** |
| 2ï¸âƒ£ | `check_connection` | Valida conectividade com o banco |
| 3ï¸âƒ£ | `bronze_extract` | Extrai produtos da FakeStoreAPI |
| 4ï¸âƒ£ | `silver_transform` | Transforma e limpa os dados |
| 5ï¸âƒ£ | `load_validate` | Valida e expÃµe os dados finais |

---

## ğŸ§  DecisÃµes TÃ©cnicas

### Uma DAG por Responsabilidade

Cada camada do pipeline Ã© uma DAG independente, permitindo monitoramento, reexecuÃ§Ã£o e extensÃ£o isolados. Os dados sÃ£o **persistidos no banco entre as camadas** (sem XCom cross-DAG):

| Tabela | Camada | ConteÃºdo |
|---|---|---|
| `bronze_products` | Bronze | Dados brutos da API (`rating` como JSONB) |
| `silver_products` | Silver | Dados transformados e tipados |

### Carga Robusta

Usa **SQLAlchemy Core (Bulk Insert)** via `save_dataframe_to_table()` em `utils.py`, evitando incompatibilidades entre Pandas `to_sql` e drivers PostgreSQL.

### IdempotÃªncia

`TRUNCATE` em transaÃ§Ã£o atÃ´mica (`engine.begin()`) garante que reexecuÃ§Ãµes nÃ£o geram dados duplicados. O setup usa `CREATE TABLE IF NOT EXISTS`.
