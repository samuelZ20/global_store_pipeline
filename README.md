# ğŸ›’ Global Store ETL Pipeline

Este projeto consiste em um **pipeline de dados automatizado** que extrai informaÃ§Ãµes de produtos da **FakeStoreAPI**, realiza transformaÃ§Ãµes de limpeza e padronizaÃ§Ã£o utilizando **Pandas**, e persiste os dados em um **Data Warehouse PostgreSQL hospedado no Render**.

O pipeline Ã© orquestrado pelo **Apache Airflow**, garantindo **idempotÃªncia** e **observabilidade** do processo.

---

## ğŸš€ Tecnologias Utilizadas

* **Linguagem:** Python 3.12
* **OrquestraÃ§Ã£o:** Apache Airflow 2.11+
* **TransformaÃ§Ã£o:** Pandas
* **Banco de Dados:** PostgreSQL (Render)
* **ConexÃ£o e Carga:** SQLAlchemy Core 1.4 (Bulk Insert)
* **Gerenciador de DependÃªncias:** Poetry

---

## ğŸ“‚ Estrutura do Projeto

```
global_store_pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ global_store_dag.py     # DefiniÃ§Ã£o do fluxo de tarefas no Airflow
â”œâ”€â”€ src/                        # MÃ³dulos de lÃ³gica do pipeline
â”‚   â”œâ”€â”€ api_client.py           # ExtraÃ§Ã£o (Camada Bronze)
â”‚   â”œâ”€â”€ transform.py            # TransformaÃ§Ã£o (Camada Silver)
â”‚   â”œâ”€â”€ db_manager.py           # Gerenciamento de conexÃ£o com banco
â”‚   â””â”€â”€ init_db.py              # DDL e inicializaÃ§Ã£o de tabelas
â”œâ”€â”€ main.py                     # ExecuÃ§Ã£o manual (Local)
â”œâ”€â”€ pyproject.toml              # DependÃªncias Poetry
â””â”€â”€ .env                        # VariÃ¡veis de ambiente (nÃ£o versionado)
```

---

## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1ï¸âƒ£ PrÃ©-requisitos

Certifique-se de ter:

* **Python 3.12**
* **Poetry**
* Ambiente Linux/WSL (recomendado para compatibilidade com o Airflow)

---

### 2ï¸âƒ£ InstalaÃ§Ã£o de DependÃªncias

```bash
poetry install
```

---

### 3ï¸âƒ£ VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto com as credenciais do banco de dados no Render:

```env
DB_USER=seu_usuario
DB_PASSWORD=sua_senha
DB_HOST=seu_host_no_render.com
DB_NAME=global_store_dw
```

---

## ğŸƒ Como Rodar

### ğŸ”¹ Modo Local (Script RÃ¡pido)

Para validar a conexÃ£o e a lÃ³gica ETL sem a interface do Airflow:

```bash
poetry run python main.py
```

---

### ğŸ”¹ Modo Orquestrado (Airflow Standalone)

Para rodar com **agendamento e monitoramento visual**:

#### 1. ConfiguraÃ§Ã£o de Caminhos

No terminal, informe ao Python a localizaÃ§Ã£o dos mÃ³dulos:

```bash
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

#### 2. Inicie o Airflow

```bash
poetry run airflow standalone
```

#### 3. Acesso

Abra o navegador em:

```
http://localhost:8080
```

Localize a DAG **`global_store_multi_task_pipeline`** e ative-a.

---

## ğŸ§  DecisÃµes TÃ©cnicas de Engenharia

### âœ… IdempotÃªncia

O processo de carga utiliza `TRUNCATE` dentro de uma transaÃ§Ã£o `engine.begin()`, garantindo que o pipeline possa ser reexecutado sem:

* duplicar dados
* deixar o banco em estado inconsistente

---

### âœ… Carga Robusta

Devido a incompatibilidades entre **Pandas** e **SQLAlchemy** em ambientes virtuais especÃ­ficos, a carga final Ã© realizada via **SQLAlchemy Core (Bulk Insert)**, contornando o erro:

```
AttributeError: Engine object has no attribute cursor
```

---

### âœ… Modularidade

A lÃ³gica Ã© separada em camadas:

* **Bronze:** ExtraÃ§Ã£o da API
* **Silver:** Limpeza e padronizaÃ§Ã£o dos dados

Essa arquitetura facilita manutenÃ§Ã£o, testes e expansÃ£o futura para novas fontes de dados.

---

## ğŸ“ Autor

**Samuel Frizzone Cardoso**
