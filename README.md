# ğŸ›’ Global Store Data Pipeline (ETL)

Este projeto consiste em um pipeline de dados automatizado para a **Global Store**, integrando extraÃ§Ã£o de APIs, transformaÃ§Ã£o de dados com Pandas e carga em um Data Warehouse na nuvem. Desenvolvido como parte dos meus estudos em CiÃªncia da ComputaÃ§Ã£o na **UFLA**.

## ğŸ—ï¸ Arquitetura (Medallion Architecture)

O fluxo de dados segue os princÃ­pios de camadas de dados:
1. **Bronze (Raw)**: ExtraÃ§Ã£o direta da FakeStoreAPI com metadados de auditoria (`api_client.py`).
2. **Silver (Clean)**: Processamento e normalizaÃ§Ã£o (flattening) do campo `rating` utilizando Pandas (`transform.py`).
3. **Load**: PersistÃªncia dos dados estruturados em um banco PostgreSQL hospedado no **Render**.


## ğŸ› ï¸ Tecnologias Utilizadas

- **Linguagem**: Python 3.12
- **GestÃ£o de DependÃªncias**: Poetry
- **Processamento**: Pandas
- **Conectividade**: SQLAlchemy / Psycopg2
- **OrquestraÃ§Ã£o**: Apache Airflow (DAGs Modulares)
- **Banco de Dados**: PostgreSQL (Render)
- **VisualizaÃ§Ã£o**: DBeaver

## ğŸ•¸ï¸ OrquestraÃ§Ã£o (Airflow)

O pipeline Ã© orquestrado por uma **DAG** modularizada em trÃªs tarefas principais:
- `extract_from_api`
- `transform_with_pandas`
- `load_to_render`

Agendamento definido para execuÃ§Ã£o diÃ¡ria (`@daily`) com sistema de retentativas automÃ¡ticas.

## ğŸš€ Como Executar

1. Clone o repositÃ³rio: `git clone https://github.com/samuelZ20/global_store_pipeline.git`
2. Instale as dependÃªncias: `poetry install`
3. Configure o arquivo `.env` com suas credenciais do banco.
4. Execute o pipeline: `poetry run python main.py`

---
**Samuel Frizzone Cardoso** Estudante de CiÃªncia da ComputaÃ§Ã£o - UFLA
